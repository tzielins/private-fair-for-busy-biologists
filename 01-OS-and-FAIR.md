---
title: "Introduction to Open Science and FAIR principles"
teaching: 35
exercises: 20
---

:::::::::::::::::::::::::::::::::::::: questions 

- What is Open Science?
- How can I benefit from Open Science?
- What are the FAIR guidelines?
- Why being FAIR matters?

::::::::::::::::::::::::::::::::::::::::::::::::

::::::::::::::::::::::::::::::::::::: objectives

- Identify parts of the Open Science movement, their goals and motivations
- Explain the main benefits of Open Science
- Recognize the barriers and risks in the adoption of Open Science practices
- Recognize typical issues that prevent data re-use
- Understand the FAIR principles


::::::::::::::::::::::::::::::::::::::::::::::::



(16 min teaching)

Science works best by exchanging ideas and building on them. Most efficient science involves both questions and
experiments being made as fully informed as possible, which requires the free exchange of data and information.

All practices that make knowledge and data **freely available** fall under the umbrella-term of **Open Science/Open
Research**. It makes science **more reproducible, transparent, and accessible**. As science becomes more open, the way
we conduct and communicate science changes continuously.

::::::::::::::::::::::::::::::::::::: callout

## What is Open Science

Open science is the movement to make scientific research (including publications, data, physical samples, and software)
and its dissemination **accessible to all levels** of an inquiring society,
amateur or professional.

Open Science represents a new approach to the scientific process based on **cooperative work**
and new ways of diffusing knowledge by **using digital technologies** and new collaborative tools.

Open science is **transparent and accessible knowledge**
that is shared and developed through collaborative networks.

Characteristics:

* Using web-based tools to facilitate information exchange and scientific collaboration
* Transparency in experimental methodology, observation, and collection of data
* Public availability and reusability of scientific data, methods and communications
* various outputs and audiences

:::::::::::::::::::::::::::::::::::::::::::::



### What is the Open Science movement?


The distribution of knowledge has always been subject to improvement.
Whilst the internet was initially developed for
military purposes, it was hijacked for communication between scientists, which provided a viable route to change the
dissemination of science.

The momentum has built up with a change in the way science is communicated to reflect what
research communities are calling for – solutions to the majority of problems (e.g. impact factors, data reusability,
reproducibility crisis, trust in the public science sector etc...) that we face today.

Open Science is the movement to increase transparency and reproducibility of research, through using
the open best practices.


![Figure 1. Open Science Building Blocks](../fig/OpenScienceBuildingBlocks.jpg)

*After [Gema Bueno de la Fuente](https://www.fosteropenscience.eu/content/what-open-science-introduction)*

### Open Science Building Blocks

* **Open Access:** Research outputs hosted in a way that make them accessible for everyone. Traditionally Open Access
  referred to journal articles, but now includes books, chapters or images.

* **Open Data:** Data freely and readily available to access, reuse, and share.
  Smaller data sets were often accessible as
  supplemental materials by journals alongside articles themselves.
  However, they should be hosted in dedicated platforms for
  more convenient and better access.

* **Open Software:** Software where the source code is made readily available;
  others are free to use, change, and
  share. Some examples of these including the coding language and supporting software R and RStudio,
  as well as image analysis software such as Fiji/ImageJ.

* **Open Notebooks:** Lab & notebooks hosted online, readily accessible to all. These are popular among some of the
  large funding bodies and allow anyone to comment on any stage of the experimental record.

* **Open Peer Review:** A system where peer review reports are published alongside the body of work. This can include
  reviewers' reports, correspondence between parties involved, rebuttals, editorial decisions etc...
  
* **Citizens Science:** Lay people become involved in scientific research, most commonly in data collection or image analysis. Platforms such as [zooniverse.org](https://www.zooniverse.org/) help connect projects with lay people interested in playing an active role in research, which  can help generate and/or process data which would otherwise be unachievable by one single person.

* **Scientific social networks:** Networks of researchers, which often meet locally in teams, but are also connected online, foster open discussions on scientific issues. Online, many people commonly use traditional social media platforms for this, such as Twitter, Instagram, various sub-reddits, discussion channels on Slack/Discord etc..., although there are also more dedicated spaces such as [researchgate.net](https://www.researchgate.net/).

* **Open Education resources:** Educational materials that are free for anyone to access and use to learn from. These can be anything from talks, instructional videos, and explanations posted on video hosting websites (e.g. YouTube), to entire digital textbooks written and then published freely online. 


### Benefits of Open Science

Possible benefits and consequences for each OS module:

**Open Access**

* speed of knowledge distribution
* leveling field for underfunded sites which otherwise wouldn’t be able to navigate the paywall
* prevent articles being paid for ‘thrice’ (first to produce, second to publish, third to access) by institutions.
* greater access to work by others, increasing chance for exposure & citations
* access to work by lay audiences, thus increases social exposure of research

**Open Data**

* ensures data isn’t lost overtime - reusability
* acceleration of scientific discovery rate
* value for money/reduced redundancy
* permits statistical re-analysis of the data to validate findings
* gives access to datasets which were not published as papers (e.g. negative results, large screening data sets)
* provides an avenue to generate new hypotheses
* permits combination of multiple data sources to address questions, provides greater power than a single data source

**Open Software**

* great source to learn programming skills
* the ability to modify creates a supportive community of users and rapid innovation
* saves time
* faster bug fixes
* better error scrutiny
* use of the same software/code allows better reproducibility between experiments
* need funds to maintain and update software

**Open Notebooks**

* 100% transparent science, allowing input from others at early stages of experiments
* source of learning about the process of how science is actually conducted
* allows access to experiments and data which otherwise never get published
* provides access to ‘negative’ results and failed experiments
* anyone, anywhere around the world, at any time, can check in on projects, including many users simultaneously
* possibility of immediate feedback
* thorough evidence of originality of ideas and experiments, negating effect of ‘scooping’

**Open Peer Review**

* visibility leads to more constructive reviews
* mitigates against editorial conflicts of interest and/or biases
* mitigates against reviewers conflicts of interest and/or biases
* allows readers to learn/benefit from comments of the reviewers

**Open Educational Materials**

* Foster collaboration between educators/others
* Show clearly how method was taught (e.g. Carpentries materials) which can be reproduces anywhere, anytime
* protects materials from becoming technologically obsolete
* authors preparing the material or contribute all earn credit (e.g. GitHub)
* recycle animations and material that is excellent (why reinvent the wheel?)

### Motivation: Money (8 min teaching)

One has to consider the moral objectives that
accompany the research/publication process: charities/taxpayers pay to fund research, these then pay again to access the
research they already funded.

From an economic point of view, scientific outputs generated by public research are a public good that everyone should be able to use at no cost.

According to EU report ["Cost-benefit analysis for FAIR research data"](https://op.europa.eu/en/publication-detail/-/publication/d375368c-1a0a-11e9-8d04-01aa75ed71a1),
€10.2bn is lost every year because of not accessible data (plus additional 16bn if accounting for re-use and research quality).

The goals of Open Science is to make research and research data available to e.g.
charities/taxpayers who funded this research.

The majority of larger UK and other countries' funding bodies are now making
Open Access publication conditional upon funding. 
As the results Open Access is adopted by majority of researcher and is the most proliferated part of the OS movement.



### Personal motivators

Open Science is advantageous to many parties involved in science (including
researcher community, funding bodies, the public even journals), which is leading to a push for the widespread adoption of
Open Science practices.

Large UK funding bodies such as The Wellcome Trust are big supporters of Open Science.
We can see with the example of Open Access, that once enforced by funders (*the stick*)
there is a wide adoption. But what about the personal motivators, *the carrots*.

::::::::::::::::::::::::::::::::::::: challenge 

## Exercise 1: Personal benefits of being "open" (4 min)

Below are some personal benefits to adopting Open Science practices.
Read through them which of them are the strongest motivators for you.
Select two the most important/attractive for you and mark them with +1,
select the two least important for you and mark them with 0

* receive higher citations
* complying with funders’ policies
* get extra value from your work (e.g. collaborators, reuse by modellers, ML specialists)
* demonstrate research impact
* save own time (reproducibility but also communication overhead)
* become pioneers
* distinguish yourself from the crowd
* plan successful research proposals
* gain valuable experience
* form community
* increased speed and/or ease of writing papers
* speed up and help with peer review
* build reputation and presence in the science community
* evidence of your scientific rigour and work ethic
* avoid embarrassment/disaster when you cannot reproduce your results

Can you think of other benefits?
How personal benefits of Open Science compare to the benefits
for the (scientific) society?

:::::::::::::::::::::::::::::::::::::

(3 min teaching)

The main difference between the public benefits of Open Science practices
and the personal motivators of outputs creators, that the public can
benefit almost instantly from the open resources.

However, the advantages for data creator comes with a delay, typically counted
in years. For example, building reputation will not happen with one dataset,
the re-use also will lead to citations/collaboration after the next research
cycle.

### DORA - declaration of Research Assessment

#### Principle of DORA:

The Declaration on Research Assessment (DORA) emerged from a recognition of the limitations and biases associated with journal-based metrics, such as Journal Impact Factors. Motivated by a desire to promote fair and transparent evaluation practices, DORA advocates for assessing research based on its intrinsic merits rather than the venue of publication. Its realization signifies a paradigm shift in how we measure the impact and significance of scholarly work, emphasizing the importance of quality, openness, and broader societal impacts.

### Adoption of Funders:

Funders worldwide are increasingly recognizing the importance of embracing DORA principles in their assessment criteria. Institutions like Wellcome and Cancer Research UK have led the charge by incorporating DORA principles into their funding applications. By prioritizing factors such as research outputs, contributions to mentorship, and plans for public engagement, these funders are signaling a commitment to supporting research that generates meaningful knowledge, fosters collaborations, and contributes to societal well-being.

### Narrative CV as a DORA-Compliant Assessment Tool:

The Narrative CV offers a DORA-compliant approach to evaluating researchers, focusing on key dimensions that reflect the principles of DORA:

-  Generation of Knowledge: Acknowledging diverse outputs such as datasets, patents, and software.
-  Development of Individuals and Collaborations: Highlighting mentorship and collaborative endeavors that enrich the research ecosystem.
-  Supporting Broader Society and the Economy: Demonstrating the societal and economic impacts of research beyond academic circles.
-  Supporting the Research Community: Engaging in open science practices and ensuring the accessibility of research outputs.

In this assessment framework, special attention is paid to Open Science practices, ensuring that research outputs are openly available to maximize their impact and visibility. Additionally, new metrics such as retweets, online views and downloads, discussions, and presence in mass media and technology platforms are considered to provide a more comprehensive understanding of research impact in today's digital age.

Embracing Open Practices and adhering to DORA principles not only aligns with ethical research conduct but also enhances the credibility and impact of scholarly work. As evidenced by the Narrative CV and the adoption of DORA principles by leading funders, the research community is moving towards a more transparent and equitable assessment paradigm. Ultimately, the choice to embrace Open Practices is not only a matter of integrity but also a recognition that authenticity and transparency are essential drivers of scientific progress. After all, as timestamps remind us, faking Open Practices is far more challenging than simply adhering to them.

### Barriers and risks of OS movement:

::::::::::::::::::::::::::::::::::::: challenge 

## Exercise 2: Why we are not doing Open Science already (4 min)

Discuss Open Science barriers, mention the reasons for not already being open:


:::::::::::::::::::::::: solution

 - sensitive data (anonymising data from administrative health records can be difficult)
 - IP
 - sensitive data
 - lack of expertise
 - the costs in $ and in time
 - novelty of data
 - lack of confidence (the fear of critics)
 - misuse (fake news)
 - it is not mandatory
 - lack of credit (publishing negative results is of little benefit to you)

::::::::::::::::::::::::

:::::::::::::::::::::::::::::::::::::

(9 min teaching)

It may seem obvious that we should adopt open science practices, but there are associated challenges with doing so.

Sensitivity of data is sometimes considered a barrier.
Shared data needs to be compliant with data privacy laws, leading
many to shy away from hosting it publicly. Anonymising data to desensitise it can help overcome this barrier.

The potential for intellectual property on research can dissuade some from adopting open practices. Again, much can be
shared if the data is filtered carefully to protect anything relating to intellectual property. 

Another risk could be seen with work on Covid19: pre-prints.
A manuscript hosted publicly prior to peer review, may
accelerate access to knowledge, but can also be misused and/or misunderstood. This can result in political and health
decision making based on faulty data, which is counter to societies’ best interest.

One concern is that opening up ones
data to the scientific community can lead to the identification of errors, which may lead to feelings of
embarrassment. However, this could be considered an upside - we should seek for our work to be scrutinized and errors to
be pointed out, and is the sign of a competent scientist.
One should rather have errors pointed out rather than risking
that irreproducible data might cause
even more embarrassment and disaster.

One of **the biggest barriers are the costs** involved in "being Open".
Firstly, making outputs readily available and usable to others takes time
and significant effort. Secondly, there are costs of hosting and storage.
For example, microscopy datasets reach sizes in terabytes,
making such data accessible for 10 years involves serious financial commitment.


# Being FAIR

We have seen how Open practices can benefit both scientific community as
a whole and individual practitioner.
The wide adoption of Open Access principles has resulted in an easy access
to recent biomedical publications.
Unfortunately, the same cannot be said about data and software
that accompanies those publications.

::::::::::::::::::::::::::::::::::::: callout

## What is data

Although scientific data is a very broad term, we still encounter
groups who (wrongly) believe they do not have data!
Data does not mean Excel files with recorded measurements from a machine.
Data also includes:

* images, not only from microscopes
* information about biological materials, like strain or patient details
* biological models
* recipes, laboratory and measurement protocols
* scripts, analysis procedures, and custom software can also be considered data

However, there are specific recommendations on how to deal with code.

::::::::::::::::::::::::::::::::::::: 

Let's have a look how challenging it can be to access and use
data from published biological papers.

::::::::::::::::::::::::::::::::::::: challenge

## Exercise 3: Impossible protocol (4 min)

You need to do a western blot to identify Titin proteins,
the largest proteins in the body, with a molecular weight of 3,800 kDa.
You found an antibody sold by Sigma Aldrich that has been validated
in western blots and immunofluorescence. Sigma Aldrich lists the
[Yu et al., 2019](https://doi.org/10.1002/acn3.50831)
paper as reference.

Find details of how to separate and transfer this large protein in
the reference paper.

* Hint 1: Methods section has a Western blot analysis subsection.  
* Hint 2: Follow the references.  

Would you say that the methods was Findable? Accessible? Reusable?

:::::::::::::::::::::::: solution

## Solution

 * Ref 17 will lead you to [this
    paper](https://doi.org/10.1002/ana.24102), which first of all is
    not Open Access
 * Access the paper through your institutions (if you can) and find
    the 'Western Blotting' protocol on page 232 which will show the
    following (Screenshot from the methods section from [Evilä et al 2014](https://doi.org/10.1002/ana.24102)):
 * ![Figure 1. Impossible Protocol](./fig/impossible_protocol.png)
 * "Western blotting were performed according to standard methods." -
    with no further reference to these standard methods, describing
    these methods, or supplementary material detailing these methods
 * This methodology is unfortunately a true dead end and we thus
     can't easily continue our experiments!

:::::::::::::::::::::::::
:::::::::::::::::::::::::::::::::::::

::::::::::::::::::::::::::::::::::::: challenge

## Impossible numbers

[Ikram 2014](https://doi.org/10.1093/jxb/err244) paper contains data about various metabolites in
different accessions (genotypes) of Arabidopsis plant. 
You would like to calculate average nitrogen content in plants grown under normal and nitrogen 
limited conditions. 
Please calculate the average (over genotypes) nitrogen content for the two experimental conditions.

* Hint 1. Data are in Supplementary data   
* Hint 2. Search for nitrogen in paper text to identify the correct data column.  

:::::::::::::::::::::::: solution

## Solution

* Finding the right table and column containing the relevant data is already problematic as the headers are obscured so they need to decoded using manuscript
* Data in pdf table so they cannot be readily used in calculations
* Depending on the software used to open (and the way the pdf was created), the local machine international settings, copying the data into Excel can bring unexpected results
![Figure 2. Pdf data copied to Excel](./fig/03-average_to_excel1.png)  
*Data needs parsing after coping to Excel*
![Figure 2. The same data copied to Excel with polish locale](./fig/03-average_to_excel1.png)  
*The same data copied to Excel with polish locale has been converted to dates*
* In general pdf tables cannot be read programmatically from R or Python.

:::::::::::::::::::::::::
:::::::::::::::::::::::::::::::::::::


(29 min teaching)


The above examples illustrate the typical challenges in accessing research
data and software. Firstly, data/protocols/software often do not have an identity
of their own, but only accompany a publication.
Second, they are not easily accessible or reusable, for example, all the details are inside one supporting information PDF file. Such file includes "printed" numerical table or even source code, both of which need to be "re-typed" if someone would like to use them. Data are shared in proprietary file format specific to a particular vendor and not accessible if one does not have a particular software that accompanies the equipment. Finally, data files are provided without detailed description
other than the whole article text.

In our examples, the protocol was difficult to **find** (the loops),
difficult to **access** (pay wall), and not **reusable** as it lacked the necessary details (dead-end).
In the second example the data were not **interoperable** and **reusable**
as their were only available as a figure graph.

To avoid such problems FAIR principles were designed.

![Figure 2. FAIR principles](./fig/FAIR.png)
After [SangyaPundir](https://commons.wikimedia.org/wiki/File:FAIR_data_principles.jpg)

::::::::::::::::::::::::::::::::::::: callout

## FAIR Principles


 In 2016, the [FAIR Guiding Principles for scientific data management and stewardship](https://www.nature.com/articles/sdata201618)
 were published in Scientific Data.
 The original guideline focused on "machine-actionability" -
 the ability of computer systems to operate on data with
 minimal human intervention. However, now the focus has shifted
 to making data accessible from a human perspective, and not an automated one
 (mostly due to the lack of user friendly tools that could help
 deal with standards and structured metadata).

 **Findable**:  Easy to find data and metadata for
 both humans and computers.
 Automatic and reliable discovery of datasets and services depends
 on machine-readable persistent identifiers (PIDs) and metadata.

 **Accessible**: (Meta)data should be retrievable by their identifier using
 a standardized and open communications protocol (including authentication
 and authorisation). Metadata should be available even when the data
 are no longer available.

 **Interoperable**: Data should be able to be combined with and used
 with other data or tools. The format of the data should be open and
 interpretable for various tools. It applies both to data and
 metadata, (meta)data should use vocabularies that follow FAIR principles.

 **Re-usable**: FAIR aims at optimizing the reuse of data.
 Metadata and data should be well-described so that they can be replicated
 and/or combined in different settings. The reuse of (meta)data
 should be stated with clear and accessible license(s).

:::::::::::::::::::::::::::::::::::::

## FAIR in biological practice

#### Findable & Accessible

Deposit data to an external, reputable public repository.

Repositories provide persistent identifiers (PIDs), catalogue options,
advanced metadata searching, and download statistics. Some repositories can also host private data or provide embargo periods, meaning access to all data can be delayed.

There are general "data agnostic" repositories, for example:

* [Dryad](http://datadryad.org),
* [Zenodo](http://zenodo.org),
* [FigShare](http://figshare.com),
* [Dataverse](http://thedata.org).

Or domain specific, for example:

* [UniProt](https://www.uniprot.org/) protein data,
* [GenBank](https://www.ncbi.nlm.nih.gov/genbank/) sequence data,
* [MetaboLights](https://www.ebi.ac.uk/metabolights/) metabolomics data
* [GitHub](https://github.com/) for code.

*We will cover repositories in more details in a later episode.*

::::::::::::::::::::::::::::::::::::: spoiler

## What are persistent identifiers (PIDs)

 A persistent identifier is a long-lasting reference to a digital resource.
 Typically it has two components:

 * a service that locates the resource over time even when its location changes
 * and a unique identifier (that distinguishes the resource or concept from others).

 Persistent identifiers aim to solve the problem of the persistence of accessing cited resource,
 particularly in the field of academic literature. All too often, web addresses (links) changes over time
 and fail to take you to the referenced resource you expected.

 There are several services and technologies (schemes) that provide PIDs
 for objects (whether digital, physical or abstract).
 One of the most popular is **Digital Object Identifier [(DOI)](https://www.doi.org/)**,
 recognizable by the prefix doi.org in the web links.
 For example: [https://doi.org/10.1038/sdata.2016.18](https://doi.org/10.1038/sdata.2016.18)
 resolves to the location of the paper that describes FAIR principles.

 Public repositories often maintain web addresses of their content in a stable form
 which follow the convention http://repository.adress/identifier;
 these are often called permalinks.
 For well establish services, permalinks can be treated as PIDs.

 For example: [http://identifiers.org/SO:0000167](http://identifiers.org/SO:0000167) resolves to a page
 defining promoter role, and can be used to annotate part of a DNA sequence
 as performing such a role during transcription.



::::::::::::::::::::::::::::::::::::: 

#### Interoperable

* Use common/free file formats (can be domain specific)
* Always use .csv or .xls files for numerical data. **Never** share data tables as word or pdf,
* Provide underlying numerical data for all plots and graphs
* Convert proprietary binary formats to the open ones.
  For example
 convert Snapgene to Genbank, microscopy multistack images to OME-TIFF

#### Reusable

1. Describe your data well / provide good metadata

 * write README file describing the data
 * user descriptive column headers for the data tables
 * tidy data tables, make them analysis friendly
 * provide as many details as possible (prepare good metadata)
 * use (meta)data formats (e.g. SBML, SBOL)
 * follow Minimum Information Standards

*Describing data in sufficient details is the most challenging part of the data sharing process.
We will cover this in more detail later on.*


2. Attach license files.

Licenses explicitly declare conditions and terms by which data and software can be re-used.
Here, we recommend:

  * for data [Creative Commons Attribution (CC BY)](https://creativecommons.org/licenses/by/4.0/)
license,
  * for code a permissive open source license such
as the [MIT](https://opensource.org/licenses/MIT),
[BSD](https://opensource.org/licenses/BSD-2-Clause),
or [Apache license](http://www.apache.org/licenses/).

::::::::::::::::::::::::::::::::::::: callout

## Copyright and data

 Software code (the text) automatically gets the default
 copyright protection
 which prevents others from copying or modifying it.
 Only by adding the explicit licence you can permit re-use by others.

 Data, being factual, cannot be copyrighted. **So why, do we need a license?**

 While the data itself cannot be copyrighted,
 the way how it is presented can be. The extend to which it is protected needs ultimately
 to be settled by the court.

 The "good actors" will restrain from using your data to avoid "court" risks.
 The "bad actors" will either ignore the risk or can afford the lawyers
 fees.

::::::::::::::::::::::::::::::::::::: 


::::::::::::::::::::::::::::::::::::: challenge 

## Exercise 4: Example of FAIR data (4 min)

 Zenodo is general data repository. 
 Have a look at the dataset record with COVID-19 data:
 [https://doi.org/10.5281/zenodo.6339631](https://doi.org/10.5281/zenodo.6339631)

 Identify how each of F.A.I.R principles has been met.  
 *Hint: navigate to linked github record to easily access the README file*


:::::::::::::::::::::::::::::::::solution

## Solution

* F: The dataset is identified by a PID (doi). It can be found by its ID. It  human accessible description and keywords, both suitable for discovery. 
* A: Data can be downloaded using standard browser.
* I: Dataset entries are in common formats: csv, R, jpg
* I: Dataset is linked to publication, github record and project website
* R: The record contains rich metadata in README file, including files structure and the detailed tables formats.
* R: Data are released under open Creative Commons Attribution Licence

:::::::::::::::::::::::::::::::::
:::::::::::::::::::::::::::::::::::::  





## FAIR and You (3 min)


 The FAIR acronym is sometimes accompanied with the following labels:
 * Findable - Citable
 * Accessible - Trackable and countable
 * Interoperable - Intelligible
 * Reusable - Reproducible


 ## Solution

 * Findable data have their own identity, so they can be easily
 cited and secure the credits to the authors
 * Data accessibility over the Internet using standard protocols can be
 easily monitored (for example using Google analytics). This results in metrics
 on data popularity or even geo-locations of data users.
 * Interoperable data can benefit the future you, for example you will be
 able to still read your data even when you no longer have access to the specialized,
 vendor specific software with which you worked with them before.
 Also the future you may not remember abreviations and ad-hoc conventions you used before
 (Intelligible).
 * Well documented data should contain all the details necessary to
 reproduce the experiments, helping the future you or someone taking over
 from you in the laboratory.
 * Saves time and money.


## FAIR vs Open Science (3 min teaching)

**FAIR does not mean Open**. Actually, FAIR guideline only requires
that the metadata record is always accessible.
For example, the existence of the data can be known (their metadata),
the data can have easy to use PID to reference them, but the actual
data files can only be downloaded after the login and authorization.

However, if data are already in the FAIR form, i.e. accessible over the internet,
in interoperable format and well documented, then it is almost
effortless to "open" the dataset and make it available to the whole public.
The data owner can do it any time when he no longer perceives oppening
as a risk.

At the same time, Open data which does not follow FAIR guidelines have
little value. If they are not well described, not in open formats then they
are not going to be re-used even if they were made "open" by posting them on some
website.



::::::::::::::::::::::::::::::::::::: challenge

## Open Science and FAIR Quiz (5 min + runs over break)

 Which of the following statements about the OS and FAIR are true/false?

* Open Science relies strongly on the Internet
* Open Access eliminates publishing costs
* Open Data facilitates re-use
* Open Data can increases confidence in research findings
* In Open Peer Review, readers vote on publication acceptance  
* Open Access permits the whole society to benefit from scientific findings
* Citizen Science engages the public in the research process
* Release of public datasets is important for career progression
* F in FAIR stands for free.
* Only figures presenting results of statistical analysis need underlying numerical data.
* Sharing numerical data as a .pdf in Zenodo is FAIR.
* Sharing numerical data as an Excel file via Github is not FAIR.
* Group website is a good place to share your data.
* Data should always be converted to Excel or .csv files in order to be FAIR.
* A DOI of a dataset helps in getting credit.
* FAIR data are peer reviewed.
* FAIR data accompany a publication.

:::::::::::::::::::::::::::::::::::: solution

## Solution

* Open Science relies strongly on the Internet T
* Open Access eliminates publishing costs F
* Open Data facilitates re-use T
* Open Data increases confidence in research findings T
* In Open Peer Review, readers vote on publication acceptance  F
* Open Access permits the whole society to benefit from scientific findings T
* Citizen Science engages the public in the research process T
* Release of public datasets is important for career progression T
* F in FAIR stands for free. F
* Only figures presenting results of statistical analysis need underlying numerical data. F
* Sharing numerical data as a .pdf in Zenodo is FAIR. F
* Sharing numerical data as an Excel file via Github is not FAIR. F
* Group website is a good place to share your data. F
* Data should always be converted to Excel or .csv files in order to be FAIR. F
* A DOI of a dataset helps in getting credit. T
* FAIR data are peer reviewed. F
* FAIR data accompany a publication. F

:::::::::::::::::::::::::::::::::::::

::::::::::::::::::::::::::::::::::::: 




### Where to next
     Further reading/links:
     - [Challenges & benefits of OS](https://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.3000246)
     - [Centre for Open Science](https://www.cos.io/)
     - [Ted talk supporting OS](https://www.youtube.com/watch?v=c-bemNZ-IqA)






## Attribution
    
     Content of this episode was adapted from:
     * Wiki [Open Science](https://en.wikipedia.org/wiki/Open_science)
     * [European Open Science Cloud](https://www.eosc-hub.eu/open-science-info)
     * [Science is necessarily collaborative - The Biochemist article](https://portlandpress.com/biochemist/article/42/3/58/225220/Science-is-necessarily-collaborative).
    

::::::::::::::::::::::::::::::::::::: keypoints

- Open Science increases transparency in research
- Publicly funded science should be publicly available
- FAIR stands for Findable Accessible Interoperable Reusable
- FAIR assures easy reuse of data underlying scientific findings

:::::::::::::::::::::::::::::::::::::::::::::::



